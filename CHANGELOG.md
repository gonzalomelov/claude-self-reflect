# Changelog

All notable changes to Claude Self-Reflect will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [2.8.2] - 2025-09-01

### Fixed
- **CRITICAL: MCP Server Startup Issue** - Resolved IndentationError preventing server initialization
  - **Root Cause**: Incorrect indentation in `update_indexing_status` function introduced during path normalization
  - **Impact**: MCP server failed to start, preventing all reflection functionality from working
  - **Solution**: Fixed function indentation while preserving path normalization improvements
  - **Files Modified**: `mcp-server/src/server.py` - Corrected lines 263-286 indentation
  - **User Action**: Update to v2.8.2 and restart Claude Code for immediate resolution

### Changed
- **Documentation Improvements**: Enhanced clarity around installation and path handling
  - Updated setup instructions with clearer Docker volume mounting guidance
  - Improved troubleshooting documentation for common installation issues
  - Better explanations of system requirements and dependencies
  - Enhanced error messaging and diagnostic information

### Technical Details
- **Validation**: MCP server startup verified, all Claude Self-Reflect tools now accessible
- **Compatibility**: Fully backward compatible, no configuration changes required
- **Performance**: No performance impact, purely a stability fix
- **Migration**: Automatic - no user action needed beyond updating package version

## [2.7.1] - 2025-08-24

### Fixed
- **CRITICAL: Reflections Not Searchable from Project Context** - Cross-agent reflection sharing now works properly
  - **Root Cause**: Reflections stored by one agent weren't searchable by the next agent when working in project context
  - **Issue**: Project-scoped searches excluded the global reflections collection, making stored insights invisible
  - **Solution**: Modified search logic to always include reflections collection when searching from specific projects
  - **Impact**: Agents can now build upon each other's stored insights and maintain conversation continuity
  - **Files Modified**:
    - `mcp-server/src/server.py`: Enhanced project-scoped search to include reflections collection
    - `mcp-server/src/server.py`: Added project metadata to newly stored reflections for better organization
    - `CLAUDE.md`: Updated documentation with reflection storage improvements
  - **Backward Compatibility**: Old reflections without project metadata remain fully searchable

### Added
- **Project Metadata for Reflections**: Newly stored reflections now include project context for better organization
- **Enhanced Search Logic**: Project-specific searches now automatically include global reflections for comprehensive results
- **Improved Cross-Agent Continuity**: Agents can now discover and build upon insights from previous interactions

### Technical Details
- **Search Behavior**: When searching from a specific project, the system now queries both the project collection and the global reflections collection
- **Metadata Schema**: New reflections include project name and context for future filtering capabilities
- **Performance**: No performance impact - reflections collection is lightweight and adds minimal overhead
- **Migration**: Automatic - no user action required, existing reflections remain accessible

### Validation
- **Cross-Agent Testing**: Verified that insights stored by one agent are discoverable by subsequent agents
- **Project Isolation**: Confirmed that project-scoped searches still work correctly while including relevant reflections
- **Backward Compatibility**: Tested that old reflections without project metadata continue to function properly

## [2.7.0] - 2025-08-21

### Added
- **Streaming import implementation** - True line-by-line JSONL processing prevents OOM on large files
- **ProjectResolver class** - Intelligent multi-strategy project name resolution with caching
- **Memory optimization** - Smart garbage collection and resource monitoring
- **Enhanced error handling** - Graceful handling of malformed JSONL entries with retry logic
- **Security improvements** - All sensitive data moved to environment variables
- **Comprehensive troubleshooting** - Enhanced diagnostic tools and error messages

### Changed
- **BREAKING**: `import-conversations-unified.py` now uses streaming implementation by default
- **BREAKING**: Docker memory limits reduced from 2GB to 600MB for production deployments
- **BREAKING**: Some container profiles disabled by default to prevent resource conflicts
- **Memory usage**: Reduced from 400MB to 150MB average (62% improvement)
- **Performance**: 15-20% faster import processing with optimized streaming
- **Container startup**: 40% faster initialization with new resource limits

### Removed
- Duplicate import scripts: `safe-watcher.py`, `parallel-streaming-importer.py`
- Backup files containing sensitive API keys from repository
- Various obsolete test scripts and temporary files
- Old batch-loading mode (was causing OOM issues)

### Fixed
- **Memory leaks**: Added `MALLOC_ARENA_MAX=2` to prevent glibc memory fragmentation
- **Docker mount path issues**: Proper state file handling across container environments
- **OOM failures**: 95% reduction in out-of-memory related failures
- **Large file processing**: Files up to 50MB+ now process successfully
- **State file conflicts**: Fixed path mismatches between Docker and host systems

### Performance
- Memory usage: 400MB â†’ 150MB average (62% reduction)
- Large file support: Successfully processes 12MB+ conversation files
- Error reduction: 95% fewer OOM-related failures
- Import speed: 15-20% performance improvement
- Resource efficiency: 70% reduction in memory requirements

### Migration Notes
- Update Docker configuration to use new 600MB memory limits
- Clean up old state files if experiencing issues
- Local embeddings now enabled by default (no API keys required)
- All existing collections remain fully accessible

## [2.6.0] - 2025-08-20

### Fixed
- **CRITICAL: Voyage AI Token Limit Exceeded** - Resolves import failures for large conversations (#38)
  - **Root Cause**: Batch size based on message count (100 messages) could exceed Voyage AI's 120,000 token limit
  - **Impact**: Some conversations with extensive code content couldn't be imported, causing data loss
  - **Solution**: Implemented intelligent token-aware batching with content analysis
  - **Files Modified**: `scripts/import-conversations-unified.py`, added architecture documentation

### Added
- **Token-Aware Batching System** for reliable Voyage AI imports:
  - Content-aware token estimation with 30% adjustment for code/JSON content
  - Dynamic batch sizing that respects 120k token limits with 20k safety buffer
  - Automatic chunk splitting for oversized conversations (max 10 recursion levels)
  - Graceful degradation with truncation warnings for single oversized messages
  - Debug logging for batch statistics and performance monitoring
- **Enhanced Configuration Options**:
  - `MAX_TOKENS_PER_BATCH` (default: 100,000) with validation bounds [1,000-120,000]
  - `TOKEN_ESTIMATION_RATIO` (default: 3 chars/token) with bounds [2-10]
  - `USE_TOKEN_AWARE_BATCHING` (default: true) for backward compatibility
  - Automatic fallback to original batching if disabled

### Changed
- **Import Reliability Improvements**:
  - Conversation chunks now analyzed for content type before batching
  - Code and JSON content detected and adjusted for higher token density
  - 10% safety margin added to all token estimates
  - Recursive chunk splitting preserves message context when possible
- **Error Handling Enhancements**:
  - Clear warnings for chunk splitting operations
  - Detailed logging of truncation events with size information
  - Graceful handling of extreme cases (stack overflow protection)

### Technical Details
- **Performance**: Minimal overhead (~1-2ms per chunk for token estimation)
- **Compatibility**: Fully backward compatible with existing installations
- **Safety**: Maximum recursion depth of 10 prevents infinite loops
- **Monitoring**: Debug logs show batch counts, sizes, and token estimates
- **Fallback**: Feature flag allows reverting to original behavior if needed

### Validation
- **Test Scenarios**: Large conversations with code blocks, JSON data, mixed content
- **Token Limits**: Verified batches stay under 100k tokens (20k buffer from 120k limit)
- **Chunk Splitting**: Tested recursive splitting preserves message boundaries
- **Error Recovery**: Confirmed graceful handling of edge cases and oversized content

### Migration Guide
No user action required - fix is automatic upon upgrade:
1. Update package: `npm install -g claude-self-reflect@2.6.0`
2. Restart import process: imports will use new token-aware batching
3. Monitor logs for any split/truncation warnings during first import

### Environment Variables
```bash
# Token limit configuration (optional)
MAX_TOKENS_PER_BATCH=100000         # Safe limit with 20k buffer
TOKEN_ESTIMATION_RATIO=3            # Conservative chars-per-token estimate
USE_TOKEN_AWARE_BATCHING=true       # Enable intelligent batching (recommended)
```

## [2.5.18] - 2025-08-17

### Security
- **Updated Dependencies**: Security patch for Docker streaming-importer
  - Updated `fastembed` from 0.2.7 to 0.4.0 (latest stable)
  - Updated `numpy` from 1.26.0 to 1.26.4 (latest compatible with fastembed constraints)
  - No critical or high severity vulnerabilities found in GitHub security scanning
  - All services tested and running correctly after updates

## [2.5.16] - 2025-08-17

### ðŸš¨ Critical Performance & Stability Release

### Fixed
- **CRITICAL: CPU Overload Issue** - Streaming importer CPU usage reduced from **1437% to <1%** (99.93% reduction)
  - **Root Cause**: Unbounded async loops without proper throttling in streaming importer
  - **Solution**: Complete rewrite with production-grade CPU monitoring and cgroup awareness
  - **Impact**: System now runs efficiently on resource-constrained environments
  - **Files Modified**: `scripts/streaming-importer.py`, `Dockerfile.streaming-importer`
- **CLI Status Command**: Fixed broken `--status` command in MCP server
  - Previously returned empty responses due to incorrect argument parsing
  - Now returns comprehensive system health including collections, memory, CPU usage
  - **Files Modified**: `mcp-server/src/server.py`

### Added
- **Production-Ready Streaming Importer** with enterprise-grade reliability:
  - Non-blocking CPU monitoring with per-core limits and cgroup detection
  - Queue overflow protection using deferred processing (data preserved, not dropped)
  - Atomic state persistence with fsync guarantees for crash recovery
  - Memory management with 15% GC buffer and automatic cleanup
  - Proper async signal handling for clean shutdowns without race conditions
  - Task cancellation on timeout preventing resource leaks
  - Exponential backoff retry logic for transient failures
  - High water mark optimization reducing filesystem scanning overhead
- **Enhanced Resource Monitoring**:
  - Real-time CPU usage tracking with container awareness
  - Memory usage monitoring with automatic garbage collection
  - Processing queue health monitoring with backlog alerts
  - Performance metrics collection and reporting

### Changed
- **V2 Token-Aware Chunking: 100% Migration Complete**
  - All collections migrated from v1 to v2 chunking format
  - Chunk configuration: 400 tokens/1600 characters with 75 token/300 character overlap
  - Search quality improved with proper semantic boundaries
  - Memory-efficient streaming chunk generation prevents OOM during processing
- **Performance Optimizations**:
  - Search response time: <3ms average, <8ms maximum across 121+ collections
  - Memory footprint: 302MB operational (60% of 500MB limit)
  - Processing rate: 4-6 files/minute stable throughput
  - Resource utilization: 96.2% memory reduction from previous versions

### Performance Metrics
| Metric | Before v2.5.16 | After v2.5.16 | Improvement |
|--------|----------------|---------------|-------------|
| CPU Usage | 1437% | <1% | 99.93% reduction |
| Memory Footprint | 8GB peak | 302MB operational | 96.2% reduction |
| Search Latency | Variable | 3.16ms avg, 7.55ms max | Consistent sub-8ms |
| Processing Success Rate | Inconsistent | 100% | Reliable |

### Technical Details
- **Test Results**: 21/25 unit tests passing for streaming importer functionality
- **Resource Management**: Semaphore-based concurrency control (embeddings: 1, Qdrant: 2)
- **State Persistence**: Atomic write operations with temporary file swapping
- **Memory Management**: Proactive garbage collection with malloc_trim on Linux
- **Queue Processing**: Oldest-first processing prevents file starvation

### Breaking Changes
- **Docker Configuration**: New environment variables required for streaming importer
- **State File Format**: Enhanced schema with additional metadata (backwards compatible)
- **Minimum Requirements**: Python 3.9+ required for async improvements

### Migration Guide
For existing installations:
1. Stop services: `docker-compose down`
2. Update docker-compose.yml with new environment variables
3. Restart: `docker-compose up -d streaming-importer`
4. Monitor: `docker stats` (CPU should be <1%)

### Environment Variables
```bash
MAX_CPU_PERCENT_PER_CORE=25        # CPU limit per core
MAX_CONCURRENT_EMBEDDINGS=1         # Embedding operations concurrency
MAX_CONCURRENT_QDRANT=2             # Qdrant operations concurrency
IMPORT_FREQUENCY=15                 # Seconds between import cycles
BATCH_SIZE=3                        # Files processed per batch
MEMORY_LIMIT_MB=400                 # Memory limit in megabytes
MAX_QUEUE_SIZE=100                  # Maximum processing queue size
```

## [2.5.10] - 2025-08-11

### Fixed
- **CRITICAL: MCP Server Startup Failure** - Emergency hotfix for IndentationError
  - **Root Cause**: Version 2.5.9 shipped with unreachable dead code after return statements in three MCP tool functions
  - **Issue**: Server failed to start with IndentationError due to dead code after return statements in:
    - `quick_search()` - 32 lines of parsing/formatting code after return statement
    - `search_summary()` - 57 lines of result analysis code after return statement  
    - `get_more_results()` - 26 lines of pagination logic after return statement
  - **Solution**: Removed all dead code after return statements while preserving error messages about MCP architectural limitations
  - **Impact**: MCP server can now start properly, reflection tools are accessible again
  - **Files Modified**: `mcp-server/src/server.py` - Removed 115+ lines of unreachable code
  - **User Action**: Update to v2.5.10 immediately to restore server functionality

### Technical Details
- **What Happened**: Functions had code after return statements that Python interpreter couldn't parse
- **Why It Happened**: Incomplete removal of old functionality when implementing MCP architectural limitation messages
- **The Fix**: Clean removal of all code after return statements in the three affected functions
- **No Functionality Lost**: The removed code was unreachable and non-functional due to return statements

## [2.5.9] - 2025-08-11

### Fixed
- **MCP Tool Interoperability**: Fixed tools attempting to call other MCP tools internally
  - **Root Cause**: MCP architecture doesn't allow tools to call other tools - only the client (Claude) can orchestrate tool calls
  - **Issue**: `quick_search`, `search_summary`, and `get_more_results` were trying to call `reflect_on_past` internally
  - **Previous Error**: "FunctionTool object is not callable" - cryptic and unhelpful
  - **Solution**: Replaced internal tool calls with graceful error messages explaining MCP architectural limitation
  - **User Guidance**: Clear alternatives provided (use reflection-specialist agent or call tools directly)
  - **Files Modified**: `mcp-server/src/server.py` - Updated 3 specialized search tools
- **Variable Scope Bug**: Fixed `cwd` variable not initialized when `project="all"` specified
  - Moved `cwd` initialization outside conditional block to ensure it's always set

### Impact
- Graceful error handling instead of cryptic "FunctionTool object is not callable" messages
- Clear guidance for users when MCP architectural limitations prevent certain operations
- Better developer experience with informative error messages
- No functional regressions - all tools work as intended within MCP constraints

## [2.5.8] - 2025-08-11

### Fixed
- **CRITICAL: Project-Scoped Search Now Works Correctly**
  - **Root Cause**: MCP server was always searching claude-self-reflect conversations regardless of which project you were actually in
  - **Issue**: The server runs from `mcp-server/` directory, so `os.getcwd()` always returned the server's directory, not Claude Code's working directory
  - **Solution**: Modified `run-mcp.sh` to capture original `$PWD` as `MCP_CLIENT_CWD` environment variable
  - **Impact**: Project-scoped search now correctly isolates conversations per project, eliminating cross-project contamination
  - **Files Modified**:
    - `run-mcp.sh`: Added `export MCP_CLIENT_CWD="$PWD"` to capture client working directory
    - `server.py`: Updated project detection logic to use `MCP_CLIENT_CWD` instead of `os.getcwd()`
    - `utils.py`: Enhanced project name normalization functions
  - **User Action**: None required - fix is automatic upon MCP server restart

## [2.5.7] - 2025-08-11

### Changed
- **Dependencies**: Removed unused `openai` package from requirements.txt
  - Package was listed but never imported or used in the codebase
  - Kept `tqdm`, `humanize`, and `backoff` for potential future use in setup wizard and rate limiting

## [2.5.6] - 2025-08-11

### Added
- **Tool Output Extraction**: Metadata v2 schema captures tool outputs and git file changes
  - Extracts up to 15 tool outputs (500 chars each) per conversation
  - Parses git diff/show/status outputs to identify modified files
  - Enables cross-agent discovery via `search_by_file` for git-modified files
  - Two-pass JSONL parsing for complete output capture
  
### Enhanced
- **Search Capabilities**: 
  - `search_by_file` now finds conversations with git-modified files
  - `search_by_concept` improved for git-related concepts
  - Tool outputs included in semantic search index
  
### Changed
- Metadata schema upgraded to version 2
  - Added `git_file_changes` field for files from git outputs
  - Added `tool_outputs` field for tool execution results
  - Backward compatible with v1 metadata
  
### Technical Details
- **Files Modified**:
  - `scripts/streaming-importer.py` - Added `extract_files_from_git_output()` function
  - `scripts/import-conversations-unified.py` - Added two-pass JSONL parsing
  - Both importers now extract tool outputs from user messages
- **Performance**: Minimal overhead (~10ms per conversation)
- **Storage**: ~2-5KB increase per conversation with tool outputs

## [2.5.5] - 2025-08-11

### Security
- **CRITICAL**: Fixed pydantic version conflict preventing MCP server startup
  - Updated pydantic from >=2.9.2 to >=2.11.7 for fastmcp 2.10.6 compatibility
  - Ensures runtime stability and prevents dependency resolution failures

### Fixed
- **Streaming Importer**: Enhanced file validation to prevent queue blocking
  - Added detection and automatic skipping of empty files (0 bytes)
  - Added detection and automatic skipping of summary-only files without conversation data
  - Implemented state tracking for skipped files to avoid re-processing
  - Prevents import pipeline from stalling on non-importable files
  - Files are re-validated if they grow in size or change modification time

### Changed
- Repository organization: Archived old release notes to docs/archive/releases/
- Cleaned up test artifacts from root directory
- Updated dependencies:
  - openai: 1.97.1 â†’ 1.98.0
  - qdrant-client: 1.15.0 â†’ 1.15.1

### Technical Details
- **Files Modified**: 
  - `scripts/streaming-importer.py` - Added comprehensive file validation functions
  - `mcp-server/pyproject.toml` - Fixed pydantic version constraint
- **State Management**: Enhanced imported-files.json to track skipped files
- **Docker**: Rebuilt streaming-importer image with validation logic
- **Impact**: Improved reliability for continuous import operations

## [2.5.1] - 2025-08-06

### Fixed
- **CRITICAL**: Collection mismatch preventing immediate search visibility of recent conversations
  - **Root Cause**: Project name extraction was using filename instead of directory name
  - **Impact**: Recent conversations stored in wrong collection (e.g., conv_7bcf787b_voyage) instead of correct one (conv_7f6df0fc_local)
  - **Fix**: Updated `normalize_project_name()` in `mcp-server/src/utils.py` to correctly extract project name from Claude logs directory structure
- Fixed streaming importer now correctly identifies projects from Claude logs paths
- Fixed project name normalization for both local and cloud embedding modes

### Changed
- Enhanced project name extraction logic to handle various Claude logs path formats:
  - Claude logs format: `-Users-kyle-Code-claude-self-reflect` -> `claude-self-reflect`
  - File paths in Claude logs: `/path/to/-Users-kyle-Code-claude-self-reflect/file.jsonl` -> `claude-self-reflect`
  - Regular file paths: `/path/to/project/file.txt` -> `project`

### Validation
- **Certified by claude-self-reflect-test agent**:
  - âœ… Local mode: Working correctly with `conv_7f6df0fc_local`
  - âœ… Cloud mode: Working correctly with `conv_7f6df0fc_voyage`
  - âœ… Memory usage: 26.9MB (47% under 50MB limit)
  - âœ… Container stability: No crashes during testing
  - âœ… Search latency: <10 seconds achieved consistently

### Performance
- Memory usage optimized: 26.9MB during operation (well under 50MB limit)
- Search latency improved: Consistent <10 second response times
- Container stability: No memory leaks or crashes detected during validation

### Technical Details
- **Files Modified**: `mcp-server/src/utils.py` - Enhanced `normalize_project_name()` function
- **Collections**: Now correctly routes conversations to appropriate collections
- **Backward Compatibility**: Existing collections remain functional
- **Migration**: No user action required - fix is automatic

## [2.4.11] - 2025-07-30

### Security
- Critical security update addressing CVE-2025-7458 (SQLite integer overflow vulnerability)
  - Updated all Docker base images from Python 3.11-slim to Python 3.12-slim
  - Added explicit `apt-get upgrade` in all Dockerfiles for system package security updates
  - Applied to all containers: importer, watcher, mcp-server, streaming-importer, importer-isolated
  - While Python 3.12 on Debian 12 still includes SQLite 3.40.1, the base image upgrade ensures better overall security posture

### Changed
- Enhanced security hardening across all Docker images with system package updates
- Fixed torch version compatibility in streaming-importer (updated to 2.3.0)
- Corrected script reference in importer-isolated to use existing unified import script

### Testing
- Comprehensive testing performed before release:
  - Local embeddings mode with FastEmbed (384-dimensional vectors)
  - Cloud embeddings mode with Voyage AI (1024-dimensional vectors)
  - Incremental import functionality (proper file change detection)
  - All Docker images build successfully except streaming-importer (known dependency issues)

## [2.4.10] - 2025-07-30

### Fixed
- Critical memory optimization: Import watcher no longer gets OOM killed on 2GB systems
  - Reduced batch size from 100 to 10 messages for lower memory footprint
  - Added per-file state saving to prevent progress loss on OOM
  - Implemented garbage collection after each file processing
  - State persistence now happens incrementally, not just at end

### Added
- Comprehensive memory footprint documentation (docs/memory-footprint.md)
  - Detailed memory usage patterns for first-time vs incremental imports
  - Troubleshooting guide for OOM issues
  - Performance metrics and optimization details
- Enhanced setup wizard with memory configuration guidance
  - Warns about first-time import memory requirements
  - Suggests temporary 4GB allocation for initial setup

### Changed
- Import process now more resilient to memory constraints
  - Works reliably with 2GB Docker memory after initial import
  - First import may still benefit from 4GB temporary allocation
- Updated documentation across multiple files:
  - README.md: Added system requirements section
  - troubleshooting.md: New memory and import issues section
  - setup-wizard-docker.js: Better memory handling guidance

### Performance
- Memory usage reduced by ~60% during import operations
- Import reliability improved - no longer fails on systems with 2GB Docker memory
- State tracking prevents re-importing unchanged files even after OOM recovery

## [2.4.9] - 2025-07-30

### Fixed
- Critical performance fix: Import watcher no longer re-imports unchanged files (#22)
  - Previously re-imported ALL files every 60 seconds causing 7+ minute cycles
  - Now tracks file modification times and skips unchanged files
  - Reduces subsequent import times from 7+ minutes to under 30 seconds
  - Significantly reduces CPU and memory usage

### Added
- Import state tracking in `.import_state.json`
  - Persists across watcher restarts
  - Tracks modification times for each imported file
  - Saves state after each project to preserve progress

### Changed
- `scripts/import-conversations-unified.py`: Added state management functions
- `Dockerfile.watcher`: Rebuilt with updated import script

### Performance
- First import: Processes all files normally
- Subsequent imports: Skip 90%+ unchanged files
- Import time: Reduced from 7+ minutes to <60 seconds
- Resource usage: Dramatically reduced CPU and memory consumption

## [2.4.8] - 2025-07-29

### Fixed
- Critical fix: Import watcher was missing scripts directory causing continuous failures
  - Updated Dockerfile.watcher to properly copy scripts directory
  - Fixed import path to use correct script location

## [2.4.7] - 2025-07-29

### Fixed
- Critical fix: Local mode now works properly without requiring Voyage API key (#21)
  - Setup wizard was incorrectly using obsolete server_v2.py module
  - Now correctly uses the main module which supports both local and Voyage embeddings
- Removed obsolete server_v2.py to prevent confusion
  - This was a temporary testing artifact whose improvements are already in server.py

### Changed
- Setup wizard Docker script now runs `python -u -m src` instead of `python -m src.server_v2`

## [2.4.5] - 2025-07-29

### Added
- Performance optimizations achieving 10-40x speed improvement
  - End-to-end response time reduced from 28.9s-2min to 2-3s
  - Response size reduced by 40-60% through compression techniques
- Brief mode parameter for minimal responses (`brief=true`)
- Progress reporting during search operations (requires client progressToken)
- Comprehensive timing logs for performance debugging
- Debug mode with `include_raw=true` for troubleshooting
- Response format parameter supporting both XML and markdown (`response_format`)

### Changed
- XML tags compressed to single letters for smaller payload (40% size reduction)
- Excerpt length optimized to 350 characters (was 500) for better context
- Default result limit kept at 5 to avoid missing relevant results
- Title and key-finding lengths optimized (80/100 chars)
- Timezone handling fixed for proper datetime comparisons

### Performance
- Search latency: 103-620ms (varies by collection count)
- MCP overhead: 75-85% of total response time
- Response size reduction: 40% (normal), 60% (brief mode)
- Streaming works properly when using reflection-specialist sub-agent

### Technical
- Added detailed timing breakdown in debug logs
- Fixed "0 tool uses" display issue with sub-agents
- Improved real-time playback with markdown format
- Better error handling for timezone-aware datetimes

### Known Issues
- Specialized search tools (`quick_search`, `search_summary`, `get_more_results`) work through the reflection-specialist agent but not via direct MCP calls due to FastMCP limitations with nested tool calls

## [2.4.4] - 2025-07-29

### Added
- XML-structured response format for reflection-specialist agent
  - All search results now return structured XML instead of markdown
  - Consistent error handling with XML format for all failure cases
  - Metadata section includes search performance metrics
  - Clear separation of summary, results, analysis, and metadata sections

### Changed
- Reflection-specialist agent completely restructured to use XML format
  - Easier parsing for main agent without regex
  - Structured data extraction with predictable field locations
  - Error responses also follow XML format for consistency

### Benefits
- Main agent can extract specific fields like `<score>`, `<project>`, `<timestamp>` directly
- No ambiguity in data boundaries or formatting
- Extensible schema allows adding new fields without breaking parsers
- Better integration capabilities for downstream tools

## [2.4.3] - 2025-07-28

### Added
- Project-scoped search functionality in MCP server (#14)
  - New optional `project` parameter for `reflect_on_past` tool
  - Default behavior: searches only current project based on working directory
  - Cross-project search with `project="all"`
  - Specific project search by name
- Comprehensive project-scoped search documentation (docs/project-scoped-search.md)
- Proactive cross-project search suggestions in reflection-specialist agent
- Project search troubleshooting section in docs
- Cross-project search strategies in advanced usage guide

### Changed
- **BREAKING**: Search behavior now defaults to project-scoped instead of searching all projects
  - Previous behavior (search all) now requires explicit `project="all"`
  - Improves focus, relevance, and performance (~100ms faster)
  - Project isolation enhances privacy between different work contexts
- Project names in search use folder names instead of full paths with dashes
- Reflection-specialist agent now indicates which project was searched

### Documentation
- Added detailed project-scoped search section to README with migration notes
- Created comprehensive guide at docs/project-scoped-search.md
- Updated reflection-specialist agent with proactive search patterns
- Enhanced advanced usage guide with cross-project strategies
- Added troubleshooting section for common project search issues

### Migration Notes
- Users upgrading from v2.4.2 or earlier will experience different search behavior
- To restore old behavior, explicitly request "search all projects" in queries
- Existing conversations remain searchable but are now filtered by project
- See [Project-Scoped Search Guide](docs/project-scoped-search.md) for details

## [2.4.2] - 2025-07-28

### Added
- Docker volume migration from bind mounts for better data persistence (PR #16)
- Exponential backoff for Voyage AI API calls with retry logic (PR #15)
- New reflect-tester agent for comprehensive system validation
- Performance baseline documentation for both embedding modes
- Tenacity library (9.1.2) for safe retry handling

### Changed
- Updated backup/restore scripts to work with Docker named volumes
- Enhanced testing infrastructure with phased approach
- Pinned all dependencies to specific versions for reproducible builds
- Fixed missing voyageai dependency in scripts/requirements.txt

### Fixed
- Agent documentation missing reflection-specialist
- Voyage AI import failures in Docker setup
- Collection naming for test projects
- Security scan false positives from accidentally committed SARIF file

### Security
- No vulnerabilities found in pip-audit scan
- All dependencies now pinned to specific versions

### Documentation
- Clarified that per-project isolation already exists (#14)
- Added workaround for npm global install path issues (#13)

## [2.4.0] - 2025-07-28

### Added
- Gitleaks configuration (`.gitleaks.toml`) for better CI/CD security scanning
- Support for handling false positives in security scans

### Changed
- Improved documentation clarity around privacy and security
- Removed unnecessary security alerts from README

### Security
- Enhanced CI/CD pipeline with proper secret scanning configuration
- Better handling of historical commits in security scans

## [2.3.7] - 2025-07-27

### Security
- Major security cleanup to reduce attack surface
- Removed archived TypeScript MCP implementation (31 files, no longer needed)
- Removed 70+ internal scripts and test files from git tracking
- Removed binary database directories (qdrant_storage/, data/) from git
- Set secure permissions (600) on .env configuration files
- Reduced codebase by ~17,000 lines and 250+ files

### Changed
- Updated .gitignore to prevent exposure of sensitive files and internal tools
- Moved internal scripts to untracked directories
- Kept only essential scripts needed for setup and validation

## [2.3.6] - 2025-07-27

### Changed
- Updated README "After" example to show actual reflection specialist sub-agent format
- Added explanation that reflection specialist is automatically spawned
- Emphasized local-first approach with optional cloud enhancement

### Documentation
- Improved clarity on how sub-agents appear in Claude's interface
- Better example using FastEmbed vs cloud decision instead of generic JWT auth

## [2.3.5] - 2025-07-27

### Changed
- Made technical documentation more neutral between local and cloud embedding options
- Removed promotional language about Voyage AI's cost-effectiveness
- Presented both embedding options equally without bias

## [2.3.4] - 2025-07-27

### Added
- Comprehensive embedding mode documentation
- Migration guide for switching between local and cloud modes
- Enhanced setup wizard with prominent embedding choice warnings
- Confirmation prompt for embedding mode selection

### Changed
- Setup wizard now clearly explains that embedding choice is semi-permanent
- Updated all documentation to emphasize the complexity of switching modes
- Improved troubleshooting guide with embedding mode issues section

### Documentation
- Created detailed embedding migration guide
- Updated installation guide with embedding mode selection
- Enhanced advanced usage guide with technical embedding details
- Added warnings throughout docs about mode switching implications

## [2.3.3] - 2025-07-27

### ðŸ”’ Security Release

### Changed
- **BREAKING**: Complete migration from sentence-transformers to FastEmbed for local embeddings
- **Default Mode**: Local embeddings now default for privacy (no external API calls)
- **Docker Memory**: Increased container memory limits to 2GB for stability
- **Security Improvements**:
  - Fixed command injection vulnerabilities in installer
  - Patched vulnerable dependencies (pydantic CVE-2024-3772)
  - Enhanced configuration security

### Added
- **Local Embeddings by Default**: Uses FastEmbed with sentence-transformers/all-MiniLM-L6-v2 model
- **Unified Import System**: Single import script supports both local and cloud embeddings
- **JSONL Parser Fix**: Proper line-by-line parsing for Claude conversation files
- **Enhanced Documentation**:
  - Privacy mode comparison table in README
  - Security update notice with GitHub info block
  - Clear warnings about data exchange in cloud mode

### Fixed
- Import failures due to incorrect JSON parsing (JSONL format)
- Memory exhaustion in Docker containers when processing large files
- MCP server initialization with local embeddings
- Reflection specialist agent support for both embedding modes

### Security
- Environment variable configuration for all sensitive settings
- Local-first approach for privacy-conscious users
- Enhanced security scanning in CI/CD pipeline

## [2.3.0] - Unreleased

### Added
- **Setup Wizard Command-Line Arguments** - Non-interactive installation support
  - `--voyage-key=<key>` for direct API key configuration
  - `--local` flag for local-only mode without semantic search
  - Automatic detection of non-interactive environments (no TTY)
- **Watcher Integration** - Continuous conversation monitoring built into setup
  - Automatic file watching after initial import
  - Real-time conversation indexing
  - Configurable watch intervals
- **System Health Dashboard** - Comprehensive health monitoring (`health-check.sh`)
  - Qdrant status with vector and collection counts
  - MCP server connection status
  - Import queue and last import timing
  - Docker container resource usage
  - Automatic update checking
  - Search performance metrics
- **Enhanced Docker Support**
  - Isolated importer container (`Dockerfile.importer-isolated`)
  - Streaming importer with progress updates (`Dockerfile.streaming-importer`)
  - Watcher container for continuous monitoring (`Dockerfile.watcher`)
  - Optimized docker-compose configurations
- **Python SSL Support** - Improved handling of Python installations
  - Automatic detection of pyenv SSL issues
  - Fallback to Homebrew Python on macOS
  - Clear error messages and remediation steps

### Changed
- **Setup Wizard Improvements**
  - Better error handling for missing dependencies
  - Clearer prompts and user feedback
  - Support for both interactive and non-interactive modes
  - Automatic Python path detection and configuration
- **Documentation Structure**
  - Reorganized docs into logical categories
  - Added troubleshooting guides for common issues
  - Improved installation instructions with platform-specific notes
  - Better examples and use cases
- **Import Process**
  - Streaming import with real-time progress
  - Better handling of large conversation files
  - Improved error recovery and retry logic
  - More efficient vector embedding batch processing

### Fixed
- **Reddit User Feedback Issues**
  - Python SSL module errors with pyenv installations
  - Non-interactive installation failures in CI/CD environments
  - Qdrant health check endpoint (changed from `/health` to `/`)
  - Docker container permission issues on Linux
  - Memory leaks in long-running import processes
- **CI/CD Pipeline**
  - Updated workflows for Python-based structure
  - Fixed test paths and dependencies
  - Added Python version matrix testing (3.10, 3.11, 3.12)
- **Setup Wizard Bugs**
  - Fixed readline interface errors in non-TTY environments
  - Corrected Python path detection on various platforms
  - Improved error messages for missing dependencies

### Security
- Better isolation of import processes using Docker
- Improved handling of sensitive configuration

## [2.2.1] - 2025-01-25

### Fixed
- NPM package installation issues
- Missing dependencies in package.json

## [2.2.0] - 2025-01-24

### Added
- Voyage AI streaming embeddings support
- Improved import performance (2x faster)
- Better error handling in setup wizard

### Changed
- Default to Voyage AI for better search accuracy
- Simplified installation process

## [2.1.0] - 2025-01-23

### Added
- Support for multiple embedding providers
- Local embedding model option
- Cross-project search functionality

### Fixed
- Memory usage optimization for large conversation sets
- Import speed improvements

## [2.0.1] - 2025-01-22

### Fixed
- CI/CD workflow for Python-based structure
- Updated test configurations
- Fixed directory references in workflows

## [2.0.0] - 2025-01-22

### Changed
- **BREAKING**: Complete restructure from TypeScript to Python MCP server
- **BREAKING**: NPM package now serves as installation wizard only
- **BREAKING**: Renamed MCP from `claude-self-reflection` to `claude-self-reflect`
- Archived TypeScript implementation to `archived/typescript-mcp/`
- Renamed directories for clarity:
  - `claude-reflect` â†’ `mcp-server`
  - `claude_reflect` â†’ `src`
- Simplified configuration structure

### Added
- New installation CLI with interactive setup wizard
- `claude-self-reflect doctor` command for diagnostics
- Python wheel distribution for MCP server
- Comprehensive migration guide

### Removed
- TypeScript MCP server implementation (archived)
- Direct MCP functionality from NPM package

### Migration Guide
Users upgrading from v1.x need to:
1. Uninstall the old package: `npm uninstall -g claude-self-reflection`
2. Install the new package: `npm install -g claude-self-reflect`
3. Run setup wizard: `claude-self-reflect setup`
4. Update Claude Desktop configuration to use `claude-self-reflect` instead of `claude-self-reflection`

## [1.0.0] - 2025-01-14

### ðŸŽ‰ Initial Release

#### Added
- **One-command installation** - Get up and running in under 5 minutes
- **Semantic search** across all Claude Desktop conversations
- **Continuous import** - Automatically watches for new conversations
- **Multiple embedding providers** - Support for OpenAI, Voyage AI, and local models
- **Cross-project search** - Search across all your Claude projects simultaneously
- **Privacy-first design** - All data stays local, no cloud dependencies
- **MCP-native integration** - Built specifically for Claude Desktop
- **Comprehensive documentation** - Full setup guide and troubleshooting
- **Health monitoring** - Built-in health check and status commands
- **Backup/restore** functionality for data safety
- **Docker-based deployment** for easy setup and isolation

#### Performance
- 66.1% search accuracy with Voyage AI embeddings
- <100ms search latency for 100K+ conversations
- ~1,000 conversations/minute import speed
- ~1GB memory usage per million conversations

#### Supported Platforms
- macOS (Apple Silicon and Intel)
- Linux (Ubuntu 20.04+, Debian 11+)
- Windows (via WSL2)

#### Known Issues
- Large conversation files (>10MB) may slow down initial import
- Search accuracy varies by embedding model choice
- Windows native support requires WSL2

### Migration from Neo4j
If you're migrating from the original Neo4j-based memento-stack:
1. Export your data using the old system's export function
2. Install Claude Self-Reflect
3. Import will automatically process your conversation history
4. All conversation data is preserved with improved search capabilities

---

## Contributing

We welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for details.

To add to this changelog:
1. Add your changes under the [Unreleased] section
2. Use the appropriate category: Added, Changed, Deprecated, Removed, Fixed, Security
3. Include PR numbers and contributor credits

Example:
```
### Added
- New feature description (#123) - @contributor
```