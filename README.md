# Claude Self-Reflect

Claude forgets everything. This fixes that.

## Table of Contents

- [What You Get](#what-you-get)
- [Requirements](#requirements)
- [Quick Install](#quick-install)
  - [Local Mode (Default)](#local-mode-default---your-data-stays-private)
  - [Cloud Mode](#cloud-mode-better-search-accuracy)
- [Uninstall Instructions](#uninstall-instructions)
- [The Magic](#the-magic)
- [Before & After](#before--after)
- [Real Examples](#real-examples-that-made-us-build-this)
- [How It Works](#how-it-works)
- [Import Architecture](#import-architecture)
- [Using It](#using-it)
- [Key Features](#key-features)
- [Performance](#performance)
- [V2.5.16 Critical Updates](#v2516-critical-updates)
- [Configuration](#configuration)
- [Technical Stack](#the-technical-stack)
- [Problems?](#problems)
- [What's New](#whats-new)
- [Advanced Topics](#advanced-topics)
- [Contributors](#contributors)

## What You Get

Ask Claude about past conversations. Get actual answers. **100% local by default** - your conversations never leave your machine. Cloud-enhanced search available when you need it.

**‚úÖ Proven at Scale**: Successfully indexed 682 conversation files with 100% reliability. No data loss, no corruption, just seamless conversation memory that works.

**Before**: "I don't have access to previous conversations"  
**After**: 
```
‚è∫ reflection-specialist(Search FastEmbed vs cloud embedding decision)
  ‚éø Done (3 tool uses ¬∑ 8.2k tokens ¬∑ 12.4s)

"Found it! Yesterday we decided on FastEmbed for local mode - better privacy, 
no API calls, 384-dimensional embeddings. Works offline too."
```

The reflection specialist is a specialized sub-agent that Claude automatically spawns when you ask about past conversations. It searches your conversation history in its own isolated context, keeping your main chat clean and focused.

Your conversations become searchable. Your decisions stay remembered. Your context persists.

## Requirements

- **Docker Desktop** (macOS/Windows) or **Docker Engine** (Linux)
- **Node.js** 16+ (for the setup wizard)
- **Claude Desktop** app

## Quick Install

### Local Mode (Default - Your Data Stays Private)
```bash
# Install and run automatic setup
npm install -g claude-self-reflect
claude-self-reflect setup

# That's it! The setup will:
# ‚úÖ Run everything in Docker (no Python issues!)
# ‚úÖ Configure everything automatically
# ‚úÖ Install the MCP in Claude Code  
# ‚úÖ Start monitoring for new conversations
# ‚úÖ Verify the reflection tools work
# üîí Keep all data local - no API keys needed
```

### Cloud Mode (Better Search Accuracy)
```bash
# Step 1: Get your free Voyage AI key
# Sign up at https://www.voyageai.com/ - it takes 30 seconds

# Step 2: Install with Voyage key
npm install -g claude-self-reflect
claude-self-reflect setup --voyage-key=YOUR_ACTUAL_KEY_HERE
```
*Note: Cloud mode provides more accurate semantic search but sends conversation data to Voyage AI for processing.*

5 minutes. Everything automatic. Just works.

## The Magic

![Self Reflection vs The Grind](docs/images/red-reflection.webp)

## Before & After

![Before and After Claude Self-Reflect](docs/diagrams/before-after-combined.webp)

## Real Examples That Made Us Build This

```
You: "What was that PostgreSQL optimization we figured out?"
Claude: "Found it - conversation from Dec 15th. You discovered that adding 
        a GIN index on the metadata JSONB column reduced query time from 
        2.3s to 45ms."

You: "Remember that React hooks bug?"
Claude: "Yes, from last week. The useEffect was missing a dependency on 
        userId, causing stale closures in the event handler."

You: "Have we discussed WebSocket authentication before?"
Claude: "3 conversations found:
        - Oct 12: Implemented JWT handshake for Socket.io
        - Nov 3: Solved reconnection auth with refresh tokens  
        - Nov 20: Added rate limiting per authenticated connection"
```

## How It Works

Your conversations ‚Üí Vector embeddings ‚Üí Semantic search ‚Üí Claude remembers

Technical details exist. You don't need them to start.

## Import Architecture

Here's how your conversations get imported and prioritized:

![Import Architecture](docs/diagrams/import-architecture.png)

**The system intelligently prioritizes your conversations:**
- **üî• HOT** (< 5 minutes): Switches to 2-second intervals for near real-time import
- **üå°Ô∏è WARM** (< 24 hours): Normal priority, processed every 60 seconds
- **‚ùÑÔ∏è COLD** (> 24 hours): Batch processed, max 5 per cycle to prevent blocking

## Using It

Once installed, just talk naturally:

- "What did we discuss about database optimization?"
- "Find our debugging session from last week"
- "Remember this solution for next time"

The reflection specialist automatically activates. No special commands needed.

## Key Features

### üéØ Project-Scoped Search
Searches are **project-aware by default**. Claude automatically searches within your current project:

```
# In ~/projects/MyApp
You: "What authentication method did we use?"
Claude: [Searches ONLY MyApp conversations]

# To search everywhere
You: "Search all projects for WebSocket implementations"
Claude: [Searches across ALL your projects]
```

### ‚è±Ô∏è Memory Decay
Recent conversations matter more. Old ones fade. Like your brain, but reliable.

### üöÄ Performance
- **Search**: <3ms average response time across 121+ collections (7.55ms max)
- **Import**: Production streaming importer with 100% reliability
- **Memory**: 302MB operational (60% of 500MB limit) - 96% reduction from v2.5.15
- **CPU**: <1% sustained usage (99.93% reduction from 1437% peak)
- **Scale**: 100% indexing success rate across all conversation types
- **V2 Migration**: 100% complete - all conversations use token-aware chunking

## V2.5.16 Critical Updates

### üö® CPU Performance Fix - RESOLVED
**Issue**: Streaming importer was consuming **1437% CPU** causing system overload  
**Solution**: Complete rewrite with production-grade throttling and monitoring  
**Result**: CPU usage reduced to **<1%** (99.93% improvement)

### ‚úÖ Production-Ready Streaming Importer
- **Non-blocking CPU monitoring** with cgroup awareness
- **Queue overflow protection** - data deferred, never dropped
- **Atomic state persistence** with fsync for crash recovery
- **Memory management** with 15% GC buffer and automatic cleanup
- **Proper async signal handling** for clean shutdowns

### üéØ 100% V2 Token-Aware Chunking
- **Complete Migration**: All collections now use optimized chunking
- **Configuration**: 400 tokens/1600 chars with 75 token/300 char overlap
- **Search Quality**: Improved semantic boundaries and context preservation
- **Memory Efficiency**: Streaming processing prevents OOM during imports

### üìä Performance Metrics (v2.5.16)
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| CPU Usage | 1437% | <1% | 99.93% ‚Üì |
| Memory | 8GB peak | 302MB | 96.2% ‚Üì |
| Search Latency | Variable | 3.16ms avg | Consistent |
| Test Success | Unstable | 21/25 passing | Reliable |

### üîß CLI Status Command Fix
Fixed broken `--status` command in MCP server - now returns:
- Collection counts and health
- Real-time CPU and memory usage
- Search performance metrics
- Import processing status

## The Technical Stack

- **Vector DB**: Qdrant (local, your data stays yours)
- **Embeddings**: 
  - Local (Default): FastEmbed with all-MiniLM-L6-v2
  - Cloud (Optional): Voyage AI
- **MCP Server**: Python + FastMCP
- **Search**: Semantic similarity with time decay

## Problems?

- [Troubleshooting Guide](docs/troubleshooting.md)
- [GitHub Issues](https://github.com/ramakay/claude-self-reflect/issues)
- [Discussions](https://github.com/ramakay/claude-self-reflect/discussions)

## What's New

- **v2.5.16** - **CRITICAL PERFORMANCE UPDATE** - Fixed 1437% CPU overload, 100% V2 migration complete, production streaming importer
- **v2.5.15** - Critical bug fixes and collection creation improvements
- **v2.5.14** - Async importer collection fix - All conversations now searchable
- **v2.5.11** - Critical cloud mode fix - Environment variables now properly passed to MCP server
- **v2.5.10** - Emergency hotfix for MCP server startup failure (dead code removal)
- **v2.5.6** - Tool Output Extraction - Captures git changes & tool outputs for cross-agent discovery

[Full changelog](docs/release-history.md)

## Advanced Topics

- [Performance tuning](docs/performance-guide.md)
- [Security & privacy](docs/security.md)
- [Windows setup](docs/windows-setup.md)
- [Architecture details](docs/architecture-details.md)
- [Contributing](CONTRIBUTING.md)

## Contributors

Special thanks to our contributors:
- **[@TheGordon](https://github.com/TheGordon)** - Fixed timestamp parsing (#10)
- **[@akamalov](https://github.com/akamalov)** - Ubuntu WSL insights
- **[@kylesnowschwartz](https://github.com/kylesnowschwartz)** - Security review (#6)

---

Stop reading. Start installing. Your future self will thank you.

MIT License. Built with ‚ù§Ô∏è for the Claude community.