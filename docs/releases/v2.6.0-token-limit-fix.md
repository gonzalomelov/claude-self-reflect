# Release Notes: v2.6.0

## Token-Aware Batching for Voyage AI Imports

This release addresses a critical issue where large conversations would fail to import due to exceeding Voyage AI's 120,000 token per batch limit. The solution implements intelligent token-aware batching that dynamically adjusts batch sizes based on content.

### What's Changed

**Core Improvements**
- Implemented token-aware batching to respect Voyage AI's 120k token limit
- Added content-aware token estimation with automatic adjustment for code and JSON content
- Introduced dynamic batch sizing that accumulates chunks until approaching token limits
- Added automatic chunk splitting for oversized content that exceeds limits

**Safety and Reliability**
- Added recursion depth limits to prevent stack overflow during chunk splitting
- Implemented environment variable validation with safe operating ranges
- Enhanced error handling with detailed truncation tracking
- Added 10% safety margin to all token estimations

**Configuration**
New environment variables for fine-tuning:
- `MAX_TOKENS_PER_BATCH`: Maximum tokens per Voyage API batch (default: 100,000)
- `TOKEN_ESTIMATION_RATIO`: Characters per token ratio (default: 3)
- `USE_TOKEN_AWARE_BATCHING`: Enable/disable token-aware batching (default: true)

### Technical Details

The implementation uses a multi-layered approach to handle various content sizes:
1. Standard chunks are batched dynamically based on cumulative token count
2. Oversized chunks are recursively split along message boundaries
3. Single oversized messages are truncated as a last resort with full tracking

Token estimation employs content-aware heuristics:
- Base estimation using configurable character-to-token ratio
- 30% adjustment for detected code or JSON content
- 10% safety margin applied to all estimates

### Backward Compatibility

The feature includes a flag (`USE_TOKEN_AWARE_BATCHING`) allowing reversion to legacy batching if needed. All changes are backward compatible with existing imports.

### Performance Impact

While the solution may create more API calls due to smaller batches, it ensures reliable imports without failures. The token counting overhead is minimal (1-2ms per chunk) and is offset by eliminating retry attempts on oversized batches.

### Monitoring and Debugging

Enhanced logging provides visibility into:
- Batch creation statistics (min/max/avg tokens and chunk counts)
- Chunk splitting operations with token estimates
- Truncation events with size information
- Environment variable validation warnings

### Migration

No migration required. The fix applies automatically to new imports while remaining compatible with existing data.

## Contributors
- @cchapman - Issue reporting and testing
- @ramakay - Implementation and testing

## Full Changelog
See commit history for detailed changes.